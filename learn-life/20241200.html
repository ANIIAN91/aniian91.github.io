<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>对2024年11月的总结 | ANIIAN'S DIARY</title>
    <meta name="description" content="Record the learning of relevant deep learning, Linux, shell, etc.">
    <meta name="generator" content="VitePress v1.0.0-rc.31">
    <link rel="preload stylesheet" href="/assets/style.80AvXVrx.css" as="style">
    
    <script type="module" src="/assets/app.DWNsumFx.js"></script>
    <link rel="preload" href="/assets/inter-roman-latin.Bu8hRsVA.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/assets/chunks/theme.Ctt0_NGz.js">
    <link rel="modulepreload" href="/assets/chunks/framework.C6kDZlj-.js">
    <link rel="modulepreload" href="/assets/learn-life_20241200.md.BvTpO5fI.lean.js">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-5a346dfe><!--[--><!--]--><!--[--><span tabindex="-1" data-v-0f60ec36></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-0f60ec36> Skip to content </a><!--]--><!----><header class="VPNav" data-v-5a346dfe data-v-ae24b3ad><div class="VPNavBar" data-v-ae24b3ad data-v-d83f3580><div class="container" data-v-d83f3580><div class="title" data-v-d83f3580><div class="VPNavBarTitle" data-v-d83f3580 data-v-86d1bed8><a class="title" href="/" data-v-86d1bed8><!--[--><!--]--><!----><!--[-->ANIIAN&#39;S DIARY<!--]--><!--[--><!--]--></a></div></div><div class="content" data-v-d83f3580><div class="curtain" data-v-d83f3580></div><div class="content-body" data-v-d83f3580><!--[--><!--]--><div class="VPNavBarSearch search" data-v-d83f3580><!----></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-d83f3580 data-v-7f418b0f><span id="main-nav-aria-label" class="visually-hidden" data-v-7f418b0f>Main Navigation</span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/index.html" tabindex="0" data-v-7f418b0f data-v-42ef59de><!--[--><span data-v-42ef59de>首页</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/learn-life/" tabindex="0" data-v-7f418b0f data-v-42ef59de><!--[--><span data-v-42ef59de>生活</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/algorithm/" tabindex="0" data-v-7f418b0f data-v-42ef59de><!--[--><span data-v-42ef59de>算法</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/model/" tabindex="0" data-v-7f418b0f data-v-42ef59de><!--[--><span data-v-42ef59de>模型</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/paper/" tabindex="0" data-v-7f418b0f data-v-42ef59de><!--[--><span data-v-42ef59de>论文</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/understanding/" tabindex="0" data-v-7f418b0f data-v-42ef59de><!--[--><span data-v-42ef59de>理解深度学习</span><!--]--></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-d83f3580 data-v-e6aabb21><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title="Switch to dark theme" aria-checked="false" data-v-e6aabb21 data-v-cbbe1149 data-v-b1685198><span class="check" data-v-b1685198><span class="icon" data-v-b1685198><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-cbbe1149><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-cbbe1149><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div><!----><div class="VPFlyout VPNavBarExtra extra" data-v-d83f3580 data-v-d0bd9dde data-v-9c007e85><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-9c007e85><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="icon" data-v-9c007e85><circle cx="12" cy="12" r="2"></circle><circle cx="19" cy="12" r="2"></circle><circle cx="5" cy="12" r="2"></circle></svg></button><div class="menu" data-v-9c007e85><div class="VPMenu" data-v-9c007e85 data-v-e7ea1737><!----><!--[--><!--[--><!----><div class="group" data-v-d0bd9dde><div class="item appearance" data-v-d0bd9dde><p class="label" data-v-d0bd9dde>Appearance</p><div class="appearance-action" data-v-d0bd9dde><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title="Switch to dark theme" aria-checked="false" data-v-d0bd9dde data-v-cbbe1149 data-v-b1685198><span class="check" data-v-b1685198><span class="icon" data-v-b1685198><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-cbbe1149><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-cbbe1149><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div></div></div><!----><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-d83f3580 data-v-e5dd9c1c><span class="container" data-v-e5dd9c1c><span class="top" data-v-e5dd9c1c></span><span class="middle" data-v-e5dd9c1c></span><span class="bottom" data-v-e5dd9c1c></span></span></button></div></div></div></div><!----></header><div class="VPLocalNav fixed reached-top" data-v-5a346dfe data-v-f84a0989><!----><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-f84a0989 data-v-1c15a60a><button data-v-1c15a60a>Return to top</button><!----></div></div><!----><div class="VPContent" id="VPContent" data-v-5a346dfe data-v-669faec9><div class="VPDoc has-aside" data-v-669faec9 data-v-6b87e69f><!--[--><!--]--><div class="container" data-v-6b87e69f><div class="aside" data-v-6b87e69f><div class="aside-curtain" data-v-6b87e69f></div><div class="aside-container" data-v-6b87e69f><div class="aside-content" data-v-6b87e69f><div class="VPDocAside" data-v-6b87e69f data-v-3f215769><!--[--><!--]--><!--[--><!--]--><div class="VPDocAsideOutline" role="navigation" data-v-3f215769 data-v-d330b1bb><div class="content" data-v-d330b1bb><div class="outline-marker" data-v-d330b1bb></div><div class="outline-title" role="heading" aria-level="2" data-v-d330b1bb>On this page</div><nav aria-labelledby="doc-outline-aria-label" data-v-d330b1bb><span class="visually-hidden" id="doc-outline-aria-label" data-v-d330b1bb> Table of Contents for current page </span><ul class="root" data-v-d330b1bb data-v-d0ee3533><!--[--><!--]--></ul></nav></div></div><!--[--><!--]--><div class="spacer" data-v-3f215769></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-6b87e69f><div class="content-container" data-v-6b87e69f><!--[--><!--]--><!----><main class="main" data-v-6b87e69f><div style="position:relative;" class="vp-doc _learn-life_20241200" data-v-6b87e69f><div><h1 id="对2024年11月的总结" tabindex="-1">对2024年11月的总结 <a class="header-anchor" href="#对2024年11月的总结" aria-label="Permalink to &quot;对2024年11月的总结&quot;">​</a></h1><p>接下来的计划是：</p><ol><li>多目标追踪与重识别的关系和一些交叉问题</li><li>重识别的具体定义和包含的一些概念</li><li>重识别与多目标追踪结合的一些问题</li><li>重识别的发展历史与历程</li><li><strong>然后尽量先把综述写出来</strong></li></ol><h2 id="多目标追踪与重识别概述" tabindex="-1">多目标追踪与重识别概述 <a class="header-anchor" href="#多目标追踪与重识别概述" aria-label="Permalink to &quot;多目标追踪与重识别概述&quot;">​</a></h2><h3 id="一-多目标追踪-mot" tabindex="-1">（一）多目标追踪（MOT） <a class="header-anchor" href="#一-多目标追踪-mot" aria-label="Permalink to &quot;（一）多目标追踪（MOT）&quot;">​</a></h3><ol><li><p><strong>定义与目标</strong><br> 多目标追踪旨在从视频序列中检测并跟踪多个目标，为每个目标赋予唯一的ID，以实现轨迹预测和行为分析。</p></li><li><p><strong>面临的挑战</strong></p><ul><li><strong>应对遮挡</strong>：追踪对象可能被其他对象或障碍物遮挡，鲁棒的追踪算法需在对象重新出现时继续追踪。</li><li><strong>处理相似对象</strong>：在存在多个相似外观对象的场景中，要正确区分和持续追踪各个独立对象。</li><li><strong>动态背景与光照变化</strong>：需在背景动态变化或光照条件差异显著的环境中有效工作。</li></ul></li></ol><h3 id="二-重识别-re-id" tabindex="-1">（二）重识别（Re-ID） <a class="header-anchor" href="#二-重识别-re-id" aria-label="Permalink to &quot;（二）重识别（Re-ID）&quot;">​</a></h3><ol><li><p><strong>定义与目标</strong><br> 重识别是在不同时间或不同摄像头下，识别并匹配同一目标的技术，主要关注目标在空间上的一致性，用于跨摄像头的目标匹配。</p></li><li><p><strong>面临的挑战</strong></p><ul><li><strong>外观变化应对</strong>：同一对象在不同摄像头、时间点或环境条件下可能出现外观变化，系统需准确识别。</li><li><strong>跨摄像头追踪</strong>：要在不同摄像头视角下正确识别并关联同一对象的不同观察实例。</li></ul></li></ol><h3 id="三-多目标追踪与重识别的关系及交叉问题" tabindex="-1">（三）多目标追踪与重识别的关系及交叉问题 <a class="header-anchor" href="#三-多目标追踪与重识别的关系及交叉问题" aria-label="Permalink to &quot;（三）多目标追踪与重识别的关系及交叉问题&quot;">​</a></h3><ol><li><p><strong>特征共享</strong><br> 重识别的特征提取方法可用于多目标追踪中的目标关联，提高跟踪准确性。</p></li><li><p><strong>联合建模</strong><br> 一些研究将两者结合，利用重识别特征增强跟踪鲁棒性，特别是在目标短暂消失或被遮挡时。</p></li></ol><h3 id="四-鲁棒性" tabindex="-1">（四）鲁棒性 <a class="header-anchor" href="#四-鲁棒性" aria-label="Permalink to &quot;（四）鲁棒性&quot;">​</a></h3><ol><li><p><strong>定义</strong><br> 模型在面对输入数据中的噪声、异常值或微小扰动时，仍能保持性能稳定、不受显著影响的能力。鲁棒性强的模型在处理含噪声或异常的数据时，能做出准确预测或分类。</p></li><li><p><strong>提高鲁棒性的方法</strong></p><ul><li><strong>数据增强</strong>：对训练数据应用变换（如旋转、缩放、裁剪等）模拟多样化场景，增强模型对异常输入的容忍性。</li><li><strong>特征工程</strong>：设计更具区分力的特征提取器，处理复杂场景和外观变化。</li><li><strong>模型融合与决策级集成</strong>：结合多个模型或算法输出提高整体准确率和鲁棒性。</li></ul></li></ol><h2 id="重识别的发展历程与贡献" tabindex="-1">重识别的发展历程与贡献 <a class="header-anchor" href="#重识别的发展历程与贡献" aria-label="Permalink to &quot;重识别的发展历程与贡献&quot;">​</a></h2><h3 id="一-早期阶段-1996-2013-基于手工特征的方法" tabindex="-1">（一）早期阶段（1996 - 2013）：基于手工特征的方法 <a class="header-anchor" href="#一-早期阶段-1996-2013-基于手工特征的方法" aria-label="Permalink to &quot;（一）早期阶段（1996 - 2013）：基于手工特征的方法&quot;">​</a></h3><ol><li><p><strong>2007年</strong><br> Farenzena等人提出基于对称性的局部特征提取方法用于行人重识别，是早期里程碑式工作。</p></li><li><p><strong>2010年</strong><br> Gray和Tao提出专门用于重识别的特征组合方法，引入行人外观描述符，结合颜色、纹理特征提升匹配效果。</p></li></ol><h3 id="二-深度学习兴起-2014-2016-基于深度学习的特征提取方法" tabindex="-1">（二）深度学习兴起（2014 - 2016）：基于深度学习的特征提取方法 <a class="header-anchor" href="#二-深度学习兴起-2014-2016-基于深度学习的特征提取方法" aria-label="Permalink to &quot;（二）深度学习兴起（2014 - 2016）：基于深度学习的特征提取方法&quot;">​</a></h3><ol><li><p><strong>2014年</strong><br> Li等人首次提出基于深度学习的Re-ID方法，利用CNN构建行人识别模型。</p></li><li><p><strong>2016年</strong><br> Zheng等人提出基准数据集Market - 1501，推动了Re-ID研究发展。</p></li></ol><h3 id="三-大规模数据集与度量学习方法-2016-至今-跨摄像头匹配和度量学习" tabindex="-1">（三）大规模数据集与度量学习方法（2016 - 至今）：跨摄像头匹配和度量学习 <a class="header-anchor" href="#三-大规模数据集与度量学习方法-2016-至今-跨摄像头匹配和度量学习" aria-label="Permalink to &quot;（三）大规模数据集与度量学习方法（2016 - 至今）：跨摄像头匹配和度量学习&quot;">​</a></h3><ol><li><p><strong>2016年</strong><br> Hermans等人提出三元损失用于Re-ID任务，提升模型性能。</p></li><li><p><strong>2018年</strong><br> Sun等人提出基于部分的特征池化方法PCB，提高行人重识别精度和鲁棒性。</p></li><li><p><strong>2019年</strong><br> Zhong等人利用域适应和样本记忆机制解决跨摄像头和跨域识别问题。</p></li></ol><h3 id="四-自监督与无监督学习的发展-2020年-至今" tabindex="-1">（四）自监督与无监督学习的发展（2020年 - 至今） <a class="header-anchor" href="#四-自监督与无监督学习的发展-2020年-至今" aria-label="Permalink to &quot;（四）自监督与无监督学习的发展（2020年 - 至今）&quot;">​</a></h3><ol><li><p><strong>2020年</strong><br> Ge等人通过自监督对比学习在无监督环境下学习有效特征表示。</p></li><li><p><strong>2021年</strong><br> Chen等人提出基于异构图对齐网络的重识别模型，提升跨域跨摄像头适用性。</p></li></ol><h3 id="五-当前的研究热点与发展方向" tabindex="-1">（五）当前的研究热点与发展方向 <a class="header-anchor" href="#五-当前的研究热点与发展方向" aria-label="Permalink to &quot;（五）当前的研究热点与发展方向&quot;">​</a></h3><ol><li><p><strong>无监督与自监督学习</strong><br> 通过对比学习、自适应域迁移等方法，让重识别模型在无标签或少标签数据上有良好表现。</p></li><li><p><strong>跨域、跨摄像头的鲁棒性</strong><br> 提升模型在不同场景、摄像头之间的泛化能力。</p></li><li><p><strong>实时性与低计算成本</strong><br> 优化模型计算速度与内存占用，适应实际监控应用。</p></li></ol><h2 id="重识别在农业领域的应用" tabindex="-1">重识别在农业领域的应用 <a class="header-anchor" href="#重识别在农业领域的应用" aria-label="Permalink to &quot;重识别在农业领域的应用&quot;">​</a></h2><h3 id="一-牲畜识别与追踪" tabindex="-1">（一）牲畜识别与追踪 <a class="header-anchor" href="#一-牲畜识别与追踪" aria-label="Permalink to &quot;（一）牲畜识别与追踪&quot;">​</a></h3><p>利用重识别技术对农场中的牲畜进行个体识别和追踪，监控其健康状况和行为模式。2023年有研究者在相关论文中探讨了深度学习在牲畜重识别中的应用，提出基于卷积神经网络的牲畜识别方法。</p><h3 id="二-农作物生长监测" tabindex="-1">（二）农作物生长监测 <a class="header-anchor" href="#二-农作物生长监测" aria-label="Permalink to &quot;（二）农作物生长监测&quot;">​</a></h3><p>通过重识别技术识别并追踪特定农作物的生长情况，评估其健康状态和生长速度。2023年有研究者提出基于深度学习的植物重识别方法用于监测农作物生长。</p><h3 id="三-鱼类识别" tabindex="-1">（三）鱼类识别 <a class="header-anchor" href="#三-鱼类识别" aria-label="Permalink to &quot;（三）鱼类识别&quot;">​</a></h3><p>在水产养殖中，用于识别和追踪个体鱼类，监控其生长和健康状况。相关研究总结在“深度学习在农业领域应用论文笔记12”中。</p><h2 id="重识别领域的专有名词和特殊概念" tabindex="-1">重识别领域的专有名词和特殊概念 <a class="header-anchor" href="#重识别领域的专有名词和特殊概念" aria-label="Permalink to &quot;重识别领域的专有名词和特殊概念&quot;">​</a></h2><ol><li><p><strong>多目标追踪（Multiple Object Tracking, MOT）</strong><br> 在视频序列中同时检测并跟踪多个目标，赋予ID并实现轨迹预测和行为分析。</p></li><li><p><strong>重识别（Re - Identification, Re - ID）</strong><br> 在不同时间或摄像头下识别并匹配同一目标，用于跨摄像头匹配。</p></li><li><p><strong>特征提取（Feature Extraction）</strong><br> 从图像或视频中提取代表目标身份的特征。</p></li><li><p><strong>度量学习（Metric Learning）</strong><br> 学习度量空间，使同一目标特征距离近，不同目标距离远。</p></li><li><p><strong>三元组损失（Triplet Loss）</strong><br> 训练模型使同一目标特征距离小于不同目标的损失函数。</p></li><li><p><strong>自监督学习（Self - Supervised Learning）</strong><br> 无需人工标注，利用数据自身结构信息学习。</p></li><li><p><strong>无监督学习（Unsupervised Learning）</strong><br> 在无标签数据下通过内在结构学习，用于领域适应和特征学习。</p></li><li><p><strong>域适应（Domain Adaptation）</strong><br> 使模型从源域知识适应目标域，提高泛化能力。</p></li><li><p><strong>多模态融合（Multi - Modal Fusion）</strong><br> 融合不同模态信息提升模型性能和鲁棒性。</p></li><li><p><strong>3D信息（3D Information）</strong><br> 利用三维传感器获取点云数据提供目标三维结构信息。</p></li></ol><p><strong>1. 多目标追踪与重识别的关系及交叉问题</strong> 多目标追踪（Multiple Object Tracking, MOT）和重识别（Re-Identification, Re-ID）在计算机视觉领域密切相关，但各自关注的重点有所不同。</p><ul><li><strong>多目标追踪（MOT）</strong>：旨在从视频序列中检测并跟踪多个目标，赋予每个目标唯一的ID，以实现轨迹预测和行为分析。MOT主要在单个摄像头的连续帧中进行，关注目标在时间上的连续性。</li><li><strong>重识别（Re-ID）</strong>：在不同时间或不同摄像头下，识别并匹配同一目标的技术。Re-ID关注目标在空间上的一致性，主要用于跨摄像头的目标匹配。</li></ul><p><strong>交叉问题</strong>：</p><ul><li><strong>特征共享</strong>：重识别的特征提取方法可用于多目标追踪中的目标关联，提高跟踪的准确性。</li><li><strong>联合建模</strong>：一些研究将重识别与多目标追踪结合，利用重识别特征增强跟踪的鲁棒性，特别是在目标短暂消失或被遮挡的情况下。</li></ul><p><strong>2. 重识别的定义及相关概念</strong></p><p>重识别（Re-Identification, Re-ID）是利用计算机视觉技术，在不同时间或不同摄像头下，识别并匹配同一目标的技术。其主要概念包括：</p><ul><li><strong>特征提取</strong>：从图像中提取能够代表目标身份的特征，如颜色、纹理、形状等。</li><li><strong>度量学习</strong>：学习一个度量空间，使得同一目标的特征距离更近，不同目标的特征距离更远。</li><li><strong>跨摄像头匹配</strong>：在不同摄像头下，考虑视角、光照、分辨率等变化，匹配同一目标。</li></ul><p><strong>3. 重识别与多目标追踪结合的问题</strong></p><p>将重识别与多目标追踪结合时，需要解决以下问题：</p><ul><li><strong>特征适配</strong>：重识别特征通常是全局特征，而多目标追踪更关注局部特征。直接应用重识别特征可能导致跟踪性能下降，需要进行特征适配。</li><li><strong>实时性</strong>：多目标追踪要求实时处理，而重识别模型通常计算复杂，需要优化以满足实时性要求。</li><li><strong>遮挡处理</strong>：在目标被遮挡或消失的情况下，如何利用重识别特征重新识别目标，是一个挑战。</li></ul><p><strong>4. 重识别的发展历史与历程</strong></p><p>重识别的研究始于20世纪90年代，随着计算机视觉和深度学习的发展，经历了以下阶段：</p><ul><li><strong>早期研究（1996-2013年）</strong>：主要基于手工特征和传统机器学习方法，研究规模较小，性能有限。</li><li><strong>深度学习兴起（2014-2016年）</strong>：深度学习方法被引入重识别领域，利用卷积神经网络（CNN）提取特征，性能显著提升。</li><li><strong>大规模数据集与评测（2016年至今）</strong>：出现了如Market-1501、DukeMTMC-reID等大规模数据集，推动了算法的发展和评测标准的建立。</li></ul><h2 id="选择性特征融合" tabindex="-1">选择性特征融合 <a class="header-anchor" href="#选择性特征融合" aria-label="Permalink to &quot;选择性特征融合&quot;">​</a></h2><p>实时语义分割任务中，卷积神经网络（CNN）通常与全卷积模型结合使用以提高精度和效率。然而，高分辨率细节与低频上下文直接融合导致细节特征容易被周围环境信息淹没。</p><h2 id="切入点和创新点" tabindex="-1">切入点和创新点 <a class="header-anchor" href="#切入点和创新点" aria-label="Permalink to &quot;切入点和创新点&quot;">​</a></h2><p><strong>切入点(Approach)</strong>： 切入点指的是你在总结论文时，如何将当前研究与已有的相关工作进行联系和过渡。它描述了你如何基于前人的工作，为本研究奠定基础，并推导出自己的研究动机和目标。切入点体现了你对前人研究的理解和继承，以及如何将自己的工作与之关联。</p><p><strong>创新点(Novelty/Contribution)</strong>： 创新点则是指你在本研究中提出的新颖性和独创性之处。它突出了你的研究与既有工作相比的差异和优势，强调了你的工作对该领域的贡献。创新点可以体现在研究方法、算法设计、应用场景、实验结果等多个方面。</p><p>总的来说： 切入点关注于如何从前人工作出发，引出并定义你自己的研究； 创新点则重点阐述你的研究中最大的亮点和创新之处。</p><h2 id="构建复杂神经网络架构时采用的模块缝合" tabindex="-1">构建复杂神经网络架构时采用的模块缝合 <a class="header-anchor" href="#构建复杂神经网络架构时采用的模块缝合" aria-label="Permalink to &quot;构建复杂神经网络架构时采用的模块缝合&quot;">​</a></h2><ol><li><strong>串行（Sequential）</strong>：数据依次流经各模块，前一模块输出为下一模块输入，类似流水线作业使输出结果逐步精细化，如经典CNN网络LeNet、VGG中卷积层和池化层多以此方式排列。</li><li><strong>并行（Parallel）</strong>：输入数据可同时送往多个模块独立处理不同任务，能提高计算效率与处理速度，在多处理器或GPU环境优势明显，像GoogLeNet中的Inception模块在同一层级有多个卷积、池化操作并行执行。</li><li><strong>交互（Interactive）</strong>：不同模块间存在反馈或信息交换机制，输出可互为输入，能让网络深层形成复杂信息交流路径，便于根据反馈调整学习及特征提取，如Transformer中的自注意力层。</li><li><strong>多尺度融合（Multi-scale Fusion）</strong>：将不同尺寸或分辨率的数据组合，以同时捕捉粗、细粒度特征，助模型在不同层级理解数据不同方面，常用于需兼顾局部细节与全局上下文的任务，例如FPN用于目标检测可改善对不同尺寸目标的检测能力。</li></ol><h2 id="线性决策边界、线性svm和非线性支持向量机" tabindex="-1">线性决策边界、线性SVM和非线性支持向量机 <a class="header-anchor" href="#线性决策边界、线性svm和非线性支持向量机" aria-label="Permalink to &quot;线性决策边界、线性SVM和非线性支持向量机&quot;">​</a></h2><h3 id="整体总结" tabindex="-1">整体总结 <a class="header-anchor" href="#整体总结" aria-label="Permalink to &quot;整体总结&quot;">​</a></h3><p>线性决策边界是一种简单直观的方式，用于划分不同类别的数据点。它通过直线（二维）或平面（高维）来区分，计算量小，但仅适用于线性可分的数据，对噪声敏感且泛化能力有限。线性SVM作为经典的二分类模型，旨在寻找最大间隔超平面，依赖少数支持向量点，并可引入软间隔，在计算效率、泛化能力及鲁棒性方面表现良好，主要针对线性可分问题。非线性支持向量机则利用核技巧将数据映射到高维空间，将非线性问题转化为线性问题，能够处理复杂的非线性情况，具有强泛化能力并可避免维度灾难，但其计算复杂度高，核函数选择与参数调优较为困难。</p><h3 id="三者区分" tabindex="-1">三者区分 <a class="header-anchor" href="#三者区分" aria-label="Permalink to &quot;三者区分&quot;">​</a></h3><p>线性决策边界仅从几何角度简单地用直线或平面区分数据，缺乏像SVM那样基于优化的边界确定机制，能力有限，仅适用于线性可分数据。线性SVM相比之下，具有严谨的数学优化目标和约束条件，科学地确定最优线性决策边界，并通过软间隔增强实用性，处理线性可分问题时更为强大可靠。非线性支持向量机的最大特点是能够处理非线性问题，通过核函数将数据“变”到高维空间，使原本在低维空间难以用直线分开的数据变得可以线性划分，但也因此带来了计算和调参的复杂性。</p><h3 id="各自特点-白话解释" tabindex="-1">各自特点（白话解释） <a class="header-anchor" href="#各自特点-白话解释" aria-label="Permalink to &quot;各自特点（白话解释）&quot;">​</a></h3><ul><li><p><strong>线性决策边界</strong>：</p><ul><li><strong>形式简单，计算量小</strong>：就像在纸上随手画条直线把不同颜色的点分开一样容易，计算过程不复杂，不像其他复杂算法需要耗费大量时间计算。</li><li><strong>只能处理线性可分的数据</strong>：只有当数据能整齐地用直线或平面分开时才有效，若数据分布混乱，无法用直线平面清晰划分，它就无能为力。</li><li><strong>对噪声数据敏感</strong>：若原本能分开的数据中有几个“捣乱”的点位置不对，原本有效的分界线可能失效，容易受异常点影响。</li><li><strong>泛化能力有限</strong>：处理已见过的数据时还行，但遇到新数据，可能表现不佳，适应新情况的能力不强。</li></ul></li><li><p><strong>线性SVM</strong>：</p><ul><li><strong>有理论保证的泛化界</strong>：从数学原理上来说，对于未见过的数据处理效果有依据、有保障，不是瞎猜的，因此在面对新数据时表现令人放心。</li><li><strong>计算效率高</strong>：运算速度快，不会让你等很久才能得到结果，处理数据时不会占用太多时间和资源。</li><li><strong>鲁棒性好</strong>：即便数据中存在一些小错误、小异常，比如个别数据标记错了，它也能稳稳地把类别分好，不会因小状况而导致分类混乱。</li></ul></li><li><p><strong>非线性支持向量机</strong>：</p><ul><li><strong>可以处理非线性问题</strong>：无论数据在原空间中形状多奇怪、分布多复杂，它都有可能通过核函数将数据“送”到高维空间，然后用直线分开，能解决棘手的非线性分类难题。</li><li><strong>泛化能力强</strong>：遇到新数据时，通常能较好地做出正确分类，适应新情况的能力突出，不像有些方法只在熟悉的数据上表现好，遇到新情况就不行。</li><li><strong>避免维度灾难</strong>：当数据维度变高时，很多算法会出问题，计算量暴增且结果不准，但它能应对高维情况，不会被高维度搞“懵”，依然能正常发挥作用。</li><li><strong>计算复杂度高</strong>：缺点是计算复杂，像完成复杂大工程，需要花费不少时间和计算资源，难以快速得出结果。</li><li><strong>核函数选择需要经验</strong>：挑选核函数如挑选合适工具干活，需有经验，不同核函数在不同数据上效果差异大，选错可能无法分好类别，需清楚数据适合用哪种核函数。</li><li><strong>参数调优困难</strong>：将内部参数调整到最佳状态不易，需反复尝试、不断摸索，如调收音机找最清晰的台，需费大劲才能达到最佳分类效果。</li></ul></li></ul></div></div></main><footer class="VPDocFooter" data-v-6b87e69f data-v-48f9bb55><!--[--><!--]--><div class="edit-info" data-v-48f9bb55><!----><div class="last-updated" data-v-48f9bb55><p class="VPLastUpdated" data-v-48f9bb55 data-v-7e05ebdb>Last updated: <time datetime="2024-12-09T01:58:37.000Z" data-v-7e05ebdb></time></p></div></div><!----></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><!----><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"algorithm_index.md\":\"DqWcpQ9I\",\"algorithm_tulun.md\":\"ix5DmbRr\",\"index.md\":\"BYlybr_m\",\"learn-life_20241121.md\":\"BZ3QWtPk\",\"learn-life_20241122.md\":\"ChohLI0r\",\"learn-life_20241123.md\":\"1w3C1jmy\",\"learn-life_20241200.md\":\"BvTpO5fI\",\"learn-life_index.md\":\"DppMxFsO\",\"learn-life_me.md\":\"BxOANnYg\",\"learn-life_school.md\":\"F1z_YRCf\",\"learn-life_school2.md\":\"D2aGB5Fy\",\"model_alexnet.md\":\"C4enAwg0\",\"model_efficientdet.md\":\"eLcBtV6s\",\"model_gan.md\":\"YU_MRrpR\",\"model_index.md\":\"DVVAv4iq\",\"model_resnet.md\":\"Bb8C5pdx\",\"paper_comparison of object detection algorithms for livestock monitoring of sheep in uav images.md\":\"D4traoJa\",\"paper_efficientdet for fabric defect detection based on edge computing.md\":\"BW-9crzv\",\"paper_efficientdet.md\":\"DnCxFzR3\",\"paper_efficientdetd0_deepsort.md\":\"Clw3dMIP\",\"paper_fast efficientdet an efficient pedestrian detection network.md\":\"DFVIwKMf\",\"paper_scalable and efficient object detection.md\":\"DVB9a-DF\",\"paper_index.md\":\"De4C9P58\",\"paper_prompts.md\":\"1R0epBZT\",\"paper_read.md\":\"CgbH3DN5\",\"paper_webpages.md\":\"BILLhu_J\",\"paper_光谱.md\":\"Dbhs-BQR\",\"paper_光谱技术在作物养分监测中的应用研究进展.md\":\"CtT6wyns\",\"paper_光谱技术在农业中的应用.md\":\"DgL8Cb7e\",\"paper_基于便携式x射线荧光光谱速测的设施菜地土壤重金属污染诊断与评价.md\":\"BbnxxKFG\",\"paper_磁力搅拌-电感耦合等离子体发射光谱测定石灰性土壤中交换性盐基.md\":\"PigUZTGp\",\"paper_适于行人重识别的二分支efficientnet网络设计.md\":\"C5b5hmEe\",\"understanding_chapter1.md\":\"DPEq-tCa\",\"understanding_chapter10.md\":\"BXtzIdXy\",\"understanding_chapter11.md\":\"BSFhnqfm\",\"understanding_chapter12.md\":\"BYkFsqtq\",\"understanding_chapter13.md\":\"Doc02Y7x\",\"understanding_chapter14.md\":\"XsjYgGT9\",\"understanding_chapter15.md\":\"DEzIh0EJ\",\"understanding_chapter16.md\":\"eWws3-wr\",\"understanding_chapter17.md\":\"qK4Ki8l6\",\"understanding_chapter18.md\":\"B0WKRjVW\",\"understanding_chapter19.md\":\"DcgENlC0\",\"understanding_chapter2.md\":\"C0F1Zb-y\",\"understanding_chapter20.md\":\"soRwvo-8\",\"understanding_chapter21.md\":\"D722F4to\",\"understanding_chapter22.md\":\"7ep3YCsA\",\"understanding_chapter3.md\":\"C77nd8J_\",\"understanding_chapter4.md\":\"CArZ8wJX\",\"understanding_chapter5.md\":\"OIA78l2Q\",\"understanding_chapter6.md\":\"BOx6NclR\",\"understanding_chapter7.md\":\"CAJVoafG\",\"understanding_chapter8.md\":\"nlQtE920\",\"understanding_chapter9.md\":\"C9IvhJfK\",\"understanding_index.md\":\"KspRdYwO\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"ANIIAN'S DIARY\",\"description\":\"Record the learning of relevant deep learning, Linux, shell, etc.\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"nav\":[{\"text\":\"首页\",\"link\":\"/index\"},{\"text\":\"生活\",\"link\":\"/learn-life/\"},{\"text\":\"算法\",\"link\":\"/algorithm/\"},{\"text\":\"模型\",\"link\":\"/model/\"},{\"text\":\"论文\",\"link\":\"/paper/\"},{\"text\":\"理解深度学习\",\"link\":\"/understanding/\"}],\"docFooter\":{\"prev\":\"上一页\",\"next\":\"下一页\"}},\"locales\":{},\"scrollOffset\":90,\"cleanUrls\":false}");</script>
    
  </body>
</html>