# 第三章：浅层神经网络内容详解

### 1. 从线性模型到神经网络的过渡 (3.1 节)

- **线性模型的局限性:** 线性模型只能表示输入和输出之间的线性关系，而现实世界中的很多问题是非线性的。为了解决这个问题，需要引入非线性函数来增强模型的表达能力。
- **引入激活函数:** 神经网络通过引入激活函数来引入非线性。激活函数是作用于线性组合结果的非线性函数，例如 **ReLU 函数** (公式 3.2)，它将负值截断为零，保留正值不变。**公式 3.1** 描述了一个具有一个输入、三个隐藏单元和一个输出的浅层神经网络的数学表达式，其中就包含了激活函数。
- **浅层神经网络:** 一个简单的浅层神经网络可以包含一个输入层、一个隐藏层和一个输出层。隐藏层中的每个神经元都应用一个激活函数。

### 2. 浅层神经网络的结构 (3.2 节)

- **术语:** 
  - **输入层:** 接收输入数据
  - **隐藏层:** 对输入数据进行非线性变换
  - **输出层:** 输出模型的预测结果
  - **前向连接:** 从输入层到隐藏层，以及从隐藏层到输出层的连接
  - **全连接网络:** 当一层中的每个变量都连接到下一层中的每个变量时，我们称之为全连接网络
  - **权重:** 每个连接都表示一个斜率参数，这些参数称为权重
  - **神经元/隐藏单元:** 隐藏层中的变量称为神经元或隐藏单元
  - **预激活:** 输入到隐藏单元的值称为预激活
  - **激活:** 应用 ReLU 函数后的隐藏单元的值称为激活
- **数学表达式:** 公式 3.1 描述了一个具有一个输入、三个隐藏单元和一个输出的浅层神经网络的数学表达式。我们可以将此计算分解为三个部分：首先，我们计算输入数据的三个线性函数 (θ10 + θ11x, θ20 + θ21x, θ30 + θ31x)。第二，我们将这三个结果传递给激活函数 a[•]。最后，我们分别使用 ϕ1、ϕ2 和 ϕ3 对三个结果激活值进行加权，将它们相加，并添加一个偏移量 ϕ0。
- **图示:** 图 3.3 展示了创建图 3.2a 中函数的计算流程。每个隐藏单元包含一个输入的线性函数 θ•0 + θ•1x，该直线在零以下被 ReLU 函数 a[•] 截断。三条线与零相交的位置成为最终输出中的三个“节点”。然后分别使用 ϕ1、ϕ2 和 ϕ3 对三条截断线进行加权。最后，添加偏移量 ϕ0，它控制最终函数的整体高度。

### 3. 激活函数的选择 (3.3 节)

- **ReLU 函数及其变体:** ReLU 函数是目前最常用的激活函数，但它存在一些问题，例如**死亡 ReLU 问题**，即当所有训练样本都产生负输入时，ReLU 函数的梯度为零，无法更新参数。为了解决这个问题，人们提出了许多 ReLU 函数的变体，例如 **Leaky ReLU**、**Parametric ReLU**、**Concatenated ReLU** 等。 
- **其他激活函数:** 除了 ReLU 函数及其变体，还有一些其他的激活函数，例如 **sigmoid 函数**、**tanh 函数**、**softplus 函数** 等。图 3.13 展示了不同激活函数的形状。
- **Swish 函数:** Swish 函数是一种通过搜索最佳激活函数得到的函数，其表达式为 a[x] = x/(1 + exp[-βx])，其中 β 是一个学习参数。
- **HardSwish 函数:** HardSwish 函数是 Swish 函数的近似，计算速度更快。

### 4. 深层神经网络 (4.3 节)

- **定义:** 深层神经网络是指具有多个隐藏层的网络。
- **网络宽度和深度:** 每层的隐藏单元数称为网络宽度，隐藏层的数量称为网络深度。
- **网络容量:** 网络的总隐藏单元数是网络容量的度量。随着隐藏单元数量的增加，模型可以逼近更复杂的函数。

### 5. 练习题

- **问题 3.8:** 要求读者绘制使用 Heaviside 阶跃函数、tanh 函数和矩形函数作为激活函数的网络结构图，并描述这些网络可以表示的函数族。原始参数为：ϕ = {ϕ0, ϕ1, ϕ2, ϕ3, θ10, θ11, θ20, θ21, θ30, θ31} = {−0.23,−1.3, 1.3, 0.66,−0.2, 0.4,−0.9, 0.9, 1.1,−0.7}。
- **问题 4.10:** 给出了一个具有两个隐藏层的深层网络的数学表达式，并要求读者解释该表达式。您可以参考图 4.5 来理解这个深层网络的计算过程。

### 总结

第三章介绍了浅层神经网络的基本概念，包括激活函数、网络结构、以及从线性模型到神经网络的过渡。此外，还简单介绍了深层神经网络的概念，为后续章节的学习奠定了基础。

## 一些额外的说明

- **万能逼近定理:** 拥有足够容量 (隐藏单元) 的浅层网络可以以任意精度描述在实线的紧凑子集上定义的任何连续一维函数。每次添加一个隐藏单元时，都会向函数添加另一个线性区域。随着这些区域变得越来越多，它们代表函数中越来越小的部分，这些部分越来越好地被一条直线近似 (图 3.5)。
- **多输出网络:** 神经网络可以有多个输出。例如，公式 3.7 和 3.8 描述了一个具有一个输入、四个隐藏单元和两个输出的网络。两个输出是隐藏单元的两个不同线性函数。由于两个输出 y1 和 y2 是相同四个隐藏单元的不同线性函数，因此每个输出中的四个“节点”必须位于相同的位置。但是，线性区域的斜率和整体垂直偏移可能会有所不同 (图 3.6)。 
- **线性函数的定义:** 在线性代数中，线性函数是指满足叠加原理的函数，即 f[a+b] = f[a]+ f[b]。这个定义意味着 f[2a] = 2f[a]。加权和 f[h1, h2, h3] = ϕ1h1 + ϕ2h2 + ϕ3h3 是线性的，但一旦添加了偏移量 (偏差)，即 f[h1, h2, h3] = ϕ0 + ϕ1h1 + ϕ2h2 + ϕ3h3，就不再满足叠加原理。为了说明这一点，请考虑在前一个函数中，当我们将参数加倍时，输出也会加倍。对于后一个函数，情况并非如此，它更准确地称为仿射函数。然而，在机器学习中，将这两个术语混淆是很常见的。在本书中，我们遵循此约定，并将两者都称为线性函数。我们将遇到的所有其他函数都是非线性的。
