## 第五章 损失函数总结

第五章主要介绍了**损失函数**的概念以及构建损失函数的方法。损失函数是用来衡量模型预测值与真实值之间差异的指标，模型训练的目标就是最小化损失函数。本章以**最大似然估计**为基础，推导出了常用的几种损失函数，并通过单变量回归、二元分类和多类别分类三个例子详细阐述了如何根据具体问题选择合适的损失函数。

* **5.1 最大似然估计 (p.56)**：本章首先介绍了**最大似然估计**的概念。最大似然估计是一种常用的参数估计方法，它通过最大化训练数据在模型预测分布下的似然来确定模型参数。根据最大似然估计，我们可以将**负对数似然**作为损失函数，并通过最小化损失函数来找到最优的模型参数。 
* **5.2 构建损失函数的步骤 (p.60)**：本节总结了构建损失函数的通用步骤：  
    1. 确定输出变量的类型和取值范围。
    2. 选择合适的概率分布来描述输出变量。
    3. 将模型的输出转换为概率分布的参数。
    4. 使用负对数似然作为损失函数。
* **5.3 例1：单变量回归 (p.61)**：本节以单变量回归为例，说明如何选择概率分布和构建损失函数。对于单变量回归问题，通常假设输出变量服从**正态分布**。损失函数可以使用**均方误差**，它等价于正态分布的负对数似然。
* **5.4 例2：二元分类 (p.64)**：本节以二元分类为例，介绍了**伯努利分布**和**逻辑sigmoid函数**。二元分类问题中，输出变量服从伯努利分布，可以使用**逻辑sigmoid函数**将模型输出映射到伯努利分布的参数。损失函数可以使用**二元交叉熵损失**。
* **5.5 例3：多类别分类 (p.67)**：本节介绍了**多类别分类**问题和**softmax函数**。多类别分类问题中，输出变量服从**类别分布**，可以使用**softmax函数**将模型输出映射到类别分布的参数。损失函数可以使用**多类别交叉熵损失**。
* **5.6 多输出 (p.69)**：本节简要介绍了多输出问题，并说明可以将单输出损失函数推广到多输出情况。
* **5.7 交叉熵损失 (p.71)**：本节对交叉熵损失函数进行了更详细的解释。
* **5.8 总结 (p.72)**：本节对本章内容进行了简要总结。

### 3. 重要概念

* **损失函数 (Loss function)**：损失函数是用来衡量模型预测值与真实值之间差异的指标，模型训练的目标就是最小化损失函数。通俗地说，损失函数就像一把尺子，用来衡量模型预测结果的好坏，损失函数值越小，说明模型预测结果越好。
* **最大似然估计 (Maximum likelihood estimation)**：最大似然估计是一种常用的参数估计方法，它通过最大化训练数据在模型预测分布下的似然来确定模型参数。通俗地说，最大似然估计就是找到一组模型参数，使得这组参数下模型生成训练数据的概率最大。
* **负对数似然 (Negative log-likelihood)**：负对数似然是最大似然估计的负数形式，通常用作损失函数。通俗地说，负对数似然就是将最大似然估计的取值范围从最大值变为最小值，方便进行模型优化。
* **伯努利分布 (Bernoulli distribution)**：伯努利分布用于描述只有两种可能结果的事件，例如抛硬币的结果。它的参数 $\lambda$ 表示事件发生的概率。
* **逻辑sigmoid函数 (Logistic sigmoid function)**：逻辑sigmoid函数将实数映射到0到1之间，可以用来将模型输出转换为伯努利分布的参数。其数学公式为：
$$
\text{sig}[z] = \frac{1}{1 + \text{exp}[-z]}.
$$
* **类别分布 (Categorical distribution)**：类别分布用于描述有多种可能结果的事件，例如掷骰子的结果。它的参数是一个向量，每个元素表示对应类别发生的概率。
* **softmax函数 (Softmax function)**：softmax函数将一个向量转换为一个概率分布，可以用来将模型输出转换为类别分布的参数。其数学公式为：
$$
\text{softmax}_k[z] = \frac{\text{exp}[z_k]}{\sum_{k'=1}^K \text{exp}[z_{k'}]}.
$$
* **交叉熵损失 (Cross-entropy loss)**：交叉熵损失用于衡量两个概率分布之间的差异，常用于分类问题。通俗地说，交叉熵损失衡量的是模型预测的概率分布与真实概率分布之间的差异程度。
