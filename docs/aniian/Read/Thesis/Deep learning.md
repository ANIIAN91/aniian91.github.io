nature14539.pdf})
[Deep learning | Nature](https://www.nature.com/articles/nature14539)

*   **英文标题:** Deep learning
*   **中文标题:** 深度学习
*   **作者:** Yann LeCun, Yoshua Bengio & Geoffrey Hinton
*   **期刊:** Nature
*   **中文关键字:** 深度学习, 卷积神经网络, 循环神经网络, 表征学习, 反向传播, 无监督学习
*   **英文关键字:** Deep learning, convolutional neural networks, recurrent neural networks, representation learning, backpropagation, unsupervised learning
*   **论文发表时间:** 2015年5月28日

**摘要:**
深度学习允许由多个处理层组成的计算模型来学习具有多个抽象级别的数据表示。这些方法极大地提高了语音识别、视觉对象识别、对象检测以及药物发现和基因组学等许多其他领域的最新技术水平。深度学习通过使用反向传播算法来指示机器应该如何改变其内部参数，这些参数用于根据前一层的表示来计算每一层中的表示。深度卷积网络在处理图像、视频、语音和音频方面取得了突破，而循环网络在处理文本和语音等序列数据方面表现出色。

**背景/目标/创新点：**
*   **背景:**
    *   传统的机器学习方法在处理原始形式的自然数据方面能力有限。
    *   构建模式识别或机器学习系统需要领域知识来设计特征提取器。

*   **目标:** 让机器能够从原始数据中自动学习特征，解决传统机器学习方法特征工程的问题。

*   **创新点/主要内容：**
    *   **多层表示:** 通过组合简单的非线性模块来获得具有多个表示级别的方法。
    *   **反向传播算法:** 用于有效地训练多层神经网络，通过计算梯度来调整网络权重。
    *   **卷积神经网络 (ConvNets):** 利用自然信号的属性（局部连接、权重共享、池化和多层结构）来处理图像、视频等数据。
    *   **循环神经网络 (RNNs):** 适用于处理序列数据，如语音和文本。
    *   **无监督学习:** 论文中简要提及了无监督学习在深度学习中的作用，并指出未来无监督学习将会更加重要。

**方法/实验设计/技术细节：**
*   **多层非线性模块:** 通过多层非线性变换，将原始数据转换为更抽象的表示。
*   **反向传播算法:**
    *   计算目标函数相对于网络权重的梯度。
    *   根据梯度调整权重，以减少误差。
*   **卷积神经网络 (ConvNets):**
    *   **卷积层:** 通过滤波器组在局部感受野上进行卷积操作，提取局部特征。
    *   **池化层:** 合并语义相似的特征，减少表示的维度，并创建对小位移和失真的不变性。
    *   **ReLU:** 常用的非线性激活函数, 有助于更快训练.
*   **循环神经网络 (RNNs):**
    *   处理序列数据，并在隐藏单元中维护一个状态向量，该向量隐含地包含有关序列过去所有元素的信息。
    *   通过时间展开，可以将 RNN 视为非常深的前馈网络，其中所有层共享相同的权重。
    *   **长短期记忆网络 (LSTM):** 引入记忆单元来解决 RNN 难以学习长期依赖关系的问题。
*   **无监督学习：**
    *   通过无标签数据训练，创建 feature detector。
    *   以逐层预训练的方式，使得网络权重可以初始化为合理的值。

**核心数据/图表/异常值:**
*   **图1:** 多层神经网络和反向传播。
*   **图2:** 卷积神经网络的内部结构。
*   **图3:** 从图像到文本。
*   **图4:** 可视化学习的单词向量。
*   **图5:** 循环神经网络及随时间推移的计算展开。

**结论解释/局限性/未来方向:**
*   **结论:** 深度学习在解决人工智能领域的难题方面取得了重大进展，并且在科学、商业和政府的许多领域都有广泛应用。
*   **未来方向:**
    *   无监督学习将变得更加重要。
    *   结合深度学习和强化学习的系统。
    *   自然语言理解。
    *   结合表示学习和复杂推理的系统。

**参考文献追踪:**
*   ImageNet Classification with Deep Convolutional Neural Networks (AlexNet)
*   Sequence to Sequence Learning with Neural Networks
*   Long Short-Term Memory

