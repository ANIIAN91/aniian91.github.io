### **Transformer介绍**：
Transformer模型的核心思想在于**完全依赖自注意力机制来计算输入和输出的表示**，从而摒弃了以往循环神经网络（RNN）或卷积神经网络（CNN）在处理序列数据时所必需的序列对齐或卷积操作 。这种变革性的设计使得Transformer模型在**处理长序列数据时展现出巨大的优势**。

相较于RNN按顺序处理数据的方式，Transformer模型能够**并行处理输入序列中的所有元素**。这种并行计算的能力极大地提升了训练和推理的效率，尤其是在面对长文本等需要处理大量序列数据的任务时。

此外，Transformer模型引入的自注意力机制**使得模型能够学习**输入序列中任意两个词语之间的**依赖关系**，无论它们在序列中的距离有多远 。这与RNN在处理长序列时可能出现的长期依赖问题形成了鲜明对比。RNN由于其固有的序列处理方式，信息在长距离传播过程中可能会逐渐衰减或丢失，导致模型难以捕捉到远距离词语之间的关联。而Transformer的注意力机制**允许每个词语直接“关注”到序列中的任何其他词语，从而更好地理解上下文信息和语义关系**.

Transformer模型与循环神经网络（RNN）和卷积神经网络（CNN）在处理序列数据上有着显著的区别。RNN以串行方式逐个处理序列中的元素，而Transformer则可以并行处理整个序列 。CNNs主要通过卷积核提取局部空间特征，虽然也可以用于处理序列数据（例如，一维卷积），但其捕捉长距离依赖的能力相对较弱 。与RNN不同，**Transformer模型本身并不具备处理序列顺序的能力，因此需要通过位置编码来显式地引入序列中元素的位置信息** .

Transformer模型的优点主要体现在以下几个方面：它能够**有效地处理长序列数据**，解决了RNN在处理长序列时面临的长期依赖问题 ；支持**并行计算**，显著缩短了模型的训练时间 ；通过注意力机制，能够**更好地捕捉序列中元素之间的上下文信息**；在自然语言处理等序列到序列的任务中表现出色 . 然而，Transformer模型也存在一些缺点，例如对于输入序列的长度可能存在一定的限制（尽管可以通过一些方法进行缓解），并且对于非常短的序列，由于其引入了额外的复杂性，可能不如RNNs高效。此外，Transformer模型本身缺乏像CNN那样的平移不变性等特性，这使得它在直接应用于视觉任务时可能需要大量的数据进行训练才能取得良好的效果。

### **Transformer架构**：
Transformer模型的架构主要由编码器（Encoder）和解码器（Decoder）两部分组成 。编码器负责将输入的序列转化为一种中间表示，而解码器（如果适用）则根据这种中间表示生成目标序列。

#### **编码器（Encoder）**
编码器由多层相同的编码器层堆叠而成。每个编码器层包含以下几个关键的子模块：
- **输入嵌入（Input Embeddings）** ：编码器的第一步是将输入的词语或子词转化为数值向量，这些向量能够捕捉到词语的语义信息 。这些嵌入向量是Transformer模型处理文本的基础，因为神经网络无法直接处理离散的符号。嵌入层将每个词语映射到一个高维空间中的稠密向量，使得语义上相似的词语在向量空间中的位置也更加接近 。这为模型理解单个词语的含义提供了初始的数值表示。
 - **位置编码（Positional Encoding）**：由于Transformer模型可以并行处理输入序列，它不像RNNs那样天然地具备处理序列顺序的能力。为了让模型理解序列中每个元素的位置信息，需要显式地添加位置编码 。位置编码通常通过使用不同频率的正弦和余弦函数生成与位置相关的向量，然后将这些向量加到输入嵌入上 。通过这种方式，Transformer模型能够区分序列中相同词语在不同位置出现的含义，从而理解句子的语法结构和语义。
- **多头自注意力机制（Multi-Head Self-Attention）**：这是Transformer模型的核心组成部分，它允许模型同时关注输入序列的不同部分，从而捕捉到更丰富的关系 。多头自注意力机制由多个并行的自注意力“头”组成，每个头都学习不同的注意力模式 。对于每个自注意力头，它会计算输入序列中每个词语的Query、Key和Value之间的关系，然后根据这些关系为每个词语生成一个加权的值向量作为输出 。多头注意力的输出会被拼接在一起，并通过一个线性层得到最终的输出。这种机制增强了模型捕捉序列内部各种复杂依赖关系的能力。
- **前馈神经网络（Feed-Forward Neural Network）**：在多头自注意力机制之后，编码器会对每个位置的表示进行独立的非线性变换，以增强模型的表达能力 。这个前馈神经网络通常包含两个线性层，中间会使用一个激活函数，例如ReLU 。前馈网络为模型引入了非线性，使其能够学习更加复杂的模式。
- **残差连接（Residual Connections）和层归一化（Layer Normalization）**：为了改善深层网络的训练效果，Transformer模型在每个子模块（例如，多头自注意力机制和前馈神经网络）的周围都使用了残差连接，并在每个子模块之后进行了层归一化 。残差连接通过将每个子层的输入直接加到其输出上，帮助缓解了在训练深层网络时可能出现的梯度消失问题 。层归一化则在每个子层之后对输出进行标准化处理，这有助于加速模型的训练过程并提高训练的稳定性 .

#### **解码器（Decoder）**
解码器（如果适用于序列到序列的任务，例如机器翻译）也由多层相同的解码器层堆叠而成。每个解码器层包含与编码器类似的子模块，但增加了一个关键的**掩码多头自注意力机制（Masked Multi-Head Self-Attention）** 。这个机制确保了解码器在生成当前位置的输出时，无法“看到”序列中未来的信息，这对于保证自回归生成过程的正确性至关重要。

此外，解码器还包含一个**编码器-解码器注意力机制（Encoder-Decoder Attention）**。这个机制允许解码器在生成输出序列的每个位置时，关注编码器的输出，从而获取输入序列的相关信息。具体来说，解码器层的自注意力机制的输出会作为Query，而编码器的输出会作为Key和Value来计算注意力。

解码器同样使用了**输出嵌入（Output Embeddings）**、**位置编码（Positional Encoding）**、**前馈神经网络（Feed-Forward Neural Network）**、**残差连接（Residual Connections）** 和 **层归一化（Layer Normalization）**，它们的作用与在编码器中类似 。

最后，解码器的最终输出会通过一个**线性层（Linear Layer）** 和一个 **Softmax层** 。线性层将解码器的输出映射到目标词汇表的大小，而Softmax层则将这些输出转化为目标词汇表上的概率分布，从而用于选择下一个生成的词语。
### Transformer流程
首先，输入的序列（例如，一个句子）会通过嵌入层和位置编码层转化为一系列的向量表示 。然后，这些向量会被输入到编码器中，编码器通过多层自注意力机制和前馈神经网络的处理，得到输入序列的上下文表示 。如果任务需要生成一个输出序列（例如，机器翻译），那么编码器的输出会作为解码器的输入之一 。解码器利用掩码自注意力机制和编码器-解码器注意力机制逐步生成输出序列 5。在每一步，解码器都会预测下一个最有可能出现的词语，直到生成一个特殊的结束标记或者达到预设的长度限制。最后，解码器的输出会通过线性层和Softmax层得到最终的输出序列

### ViT介绍
Vision Transformer（ViT）模型的核心思想是将输入的**图像分割成一系列固定大小的图像块**，然后将这些图像块视为序列数据中的“词语”，输入到标准的Transformer编码器中进行处理 。它们之间的区别：**ViT模型的输入是图像块的序列，而原始Transformer模型的输入是词语或子词的序列**。

Vision Transformer（ViT）架构的核心思想是将输入图像视为一个序列，并利用标准的Transformer编码器来处理这个序列 。其整体流程主要包括将图像转化为序列、通过Transformer编码器进行处理以及最终的分类预测等步骤

ViT在计算效率和准确性方面甚至超越了当前最先进的CNN 。ViT通过将图像视为一系列图像块，如同文本Transformer处理词嵌入序列一样，直接预测图像的类别标签 。


### ViT架构
- **图像分块（Image Patching）和线性嵌入（Linear Embedding）**：首先，输入的高分辨率图像会被分割成大小相等的非重叠图像块，例如，一个典型的尺寸是16x16像素 。然后，每个图像块会被展平成一个一维的向量，并通过一个线性层将其映射到固定维度的嵌入向量。这个过程将图像转换成了一个可以被Transformer处理的序列。

- **位置编码（Positional Encoding）**：与Transformer模型类似，由于ViT模型将图像视为一个序列进行处理，因此需要添加位置编码来保留每个图像块在原始图像中的空间位置信息 。通常，可以将标准的位置编码直接加到图像块的嵌入向量上 。这使得模型能够理解图像块之间的相对位置关系，从而更好地理解图像的整体结构。

 - **Transformer编码器（Transformer Encoder）**：经过分块、嵌入和位置编码处理后的图像块序列会被输入到一个标准的Transformer编码器中。这个编码器由多个相同的编码器层堆叠而成，每个编码器层都包含**多头自注意力机制、层归一化、前馈神经网络和残差连接** 。自注意力机制允许模型学习图像中不同位置的图像块之间的全局关系。
	 - **Multi-Head Attention (多头注意力):** 这是 Transformer Encoder 的核心组件，用于捕捉输入序列中不同位置之间的关系。
	 - **Residual Connection(残差连接)**： 它将多头自注意力机制的输入 (经过 Layer Norm 后的) 直接加到多头自注意力机制的输出上。 这有助于解决梯度消失问题。
	 - **Layer Normalization(层归一化)**：有助于加速训练，提高模型稳定性也分P**re-LN 和 Post-LN**。
	 - **Multilayer Perceptron（多层感知机):** 这表示前馈神经网络，通常是一个两层的全连接神经网络，中间有一个激活函数，它对每个位置的表示进行进一步的非线性变换（GELU）

- **分类头（Classification Head）**：为了进行图像分类，通常会在图像块序列的开头添加一个特殊的**分类标记（** **token）** 。这个标记与其他图像块一样，也会通过Transformer编码器进行处理。在编码器的最后一层，该标记的输出会被用作整个图像的表示，并输入到一个简单的多层感知机（MLP）分类器中进行最终的分类预测。


### ViT流程
首先，输入的图像被分割成一系列的图像块，然后对这些图像块进行线性嵌入和位置编码。接着，包含标记的嵌入序列会被输入到Transformer编码器中 。编码器通过自注意力机制学习图像块之间的全局关系，使得每个图像块的表示都能够考虑到图像中其他所有图像块的信息 。最后 Transformer编码器最后一层的标记的输出会被传递到分类头进行图像分类

1.  **图像分块 (Patch Partitioning):** 将 224x224 的图像分割成 196 个图像块 (patch_size=16)。
2.  **线性嵌入 (Linear Embedding of Patches):** 将这 196 个图像块分别嵌入到 `embedding_dim` 维度的向量空间中，得到 196 个嵌入向量。
3.  **添加 `[CLS]` Token:**  **在这里添加 `[CLS]` token**。  `[CLS]` token 是一个可学习的向量，其维度也为 `embedding_dim`。  通常将它添加到 196 个嵌入向量的序列的开头。  因此，现在的序列长度变为 197 (196 + 1)。
4.  **位置编码 (Position Embedding):** 为序列中的每个 token (包括 `[CLS]` token) 添加位置编码。  由于现在序列长度为 197，因此需要一个长度为 197 的位置编码向量。
5.  **Transformer Encoder:**  将包含 `[CLS]` token 和位置编码的序列送入 Transformer Encoder。
6.  **分类头 (Classification Head):**  从 Transformer Encoder 的输出中提取对应于 `[CLS]` token 的向量，并将其送入分类头进行分类。


### ViT和CNN
ViT模型的优点在于它能够有效地捕捉图像中的全局依赖关系，这与传统的CNN模型主要关注局部特征有所不同 。在足够大的数据集上进行预训练后，ViT模型可以取得非常好的性能，甚至在某些视觉任务上超过一些先进的CNN模型 。

然而，ViT模型也存在一些缺点。相比于CNN，ViT的归纳偏置较弱，这意味着它对于图像的先验知识较少，因此通常需要更多的数据进行训练才能达到最佳性能 。在小数据集上，ViT模型容易发生过拟合 。此外，由于自注意力机制的计算复杂度与序列长度（即图像块的数量）的平方成正比，当输入图像尺寸较大时，ViT模型的计算成本可能会比较高 。

卷积操作的创归纳偏置（每一层）： 局部性 、平移不变性 
VT的归纳偏置（仅在切分patch时）： 切分Patch时引入了局部性 、多个Patch,用同一个线性映射层，引入了平移不变性