## ViT的详细组成部分和流程

### 图像分块和嵌入
给定一个尺寸为H x W x C（高度 x 宽度 x 通道数）的输入图像，ViT首先将其分割成N个大小为P x P x C的固定大小的图像块，其中N = (H/P) x (W/P)，P是用户设定的图像块大小 。例如，对于一个224x224像素的RGB图像，如果选择的图像块大小为16x16，那么图像将被分割成(224/16) x (224/16) = 14x14 = 196个图像块。每个P x P x C的图像块会被展平成一个D维的向量，其中D = P * P * C 。在上述例子中，每个16x16x3的图像块将被展平成一个16 * 16 * 3 = 768维的向量。

这些D维的扁平化图像块向量随后会通过一个可学习的线性层（可以是一个全连接层，也可以是一个卷积核大小和步长都等于P的卷积层）进行线性投影，将其映射到一个E维的嵌入空间中 。这个过程产生了N个图像块的嵌入向量，每个向量的维度都是E。图像块的大小P是一个关键的超参数。较小的图像块尺寸可以捕获更细粒度的图像细节，但会导致序列长度增加，从而增加计算成本。相反，较大的图像块尺寸虽然计算效率更高，但可能会丢失一些细微的信息 。图像块嵌入的实现通常使用卷积层，其卷积核大小和步长都设置为图像块的大小，然后进行扁平化操作，这种方法在计算上很高效，并且可以利用GPU的并行处理能力。另一种方法是手动创建图像块然后再进行扁平化 。

**首先，将输入图像分割成固定大小的图像块（patches）。然后，每个图像块通过线性投影（一个全连接层，或者等价于 1x1 卷积）被转换成一个嵌入向量。**

**然后在所有图像块的嵌入向量序列的_开头_，添加一个可学习的 CLS Token 的嵌入向量。 这个 CLS Token 的嵌入向量是随机初始化的，然后在训练过程中学习，他的作用是汇总整个图像的信息**

### 位置编码
与循环神经网络（RNN）不同，Transformer模型本身并不具备处理序列顺序的能力，因为它是并行处理所有输入令牌的 。因此，为了让ViT能够理解图像块在原始图像中的空间排列顺序，需要向每个图像块的嵌入向量添加位置编码 。位置编码向量的维度与图像块嵌入向量的维度E相同。

位置编码的生成方法可以是固定的，例如使用原始Transformer论文中提出的正弦和余弦函数，也可以是在训练过程中学习得到的 。大多数ViT的实现都采用可学习的位置编码 。对于每个图像块在序列中的位置（对应于其在原始图像中的位置），都会学习到一个E维的位置编码向量。这个位置编码向量会与对应的图像块嵌入向量进行逐元素相加 。可学习的位置编码使得模型能够根据具体的任务和数据集自适应地学习空间关系的表示。实验表明，相邻图像块的嵌入向量在训练后会表现出较高的相似性，这说明模型有效地学习了图像的底层网格结构。

### Transformer编码器
Transformer编码器是ViT架构的核心部分，它由多个相同的编码器层堆叠而成（例如，12层、24层）。每个编码器层包含两个主要的子层：多头自注意力（MSA）机制和一个前馈网络（FFN），这两个子层都围绕着层归一化（LN）和残差连接 。

**多头自注意力（MSA）**：自注意力机制是Transformer的关键创新。它允许序列中的每个图像块关注序列中的所有其他图像块，并根据它们之间的相关性计算出一个注意力得分。具体来说，对于每个输入的嵌入向量（图像块嵌入 + 位置编码），通过三个不同的线性变换得到查询（Query，Q）、键（Key，K）和值（Value，V）向量 。图像块i和图像块j之间的注意力得分通过计算查询向量Q<sub>i</sub>和键向量K<sub>j</sub>的点积得到，然后除以键向量维度的平方根（√d<sub>k</sub>，用于稳定梯度），最后通过一个Softmax函数进行归一化，得到注意力权重 。自注意力的计算公式如下：
					Attention(Q, K, V) = softmax((QK<sup>T</sup>) / √d<sub>k</sub>)V

自注意力机制的输出是所有图像块的值向量的加权和，权重就是计算得到的注意力得分 。为了能够捕获不同方面的关联信息，多头自注意力机制并行地使用多个独立的注意力头进行计算 。每个注意力头的输出会被拼接起来，并通过一个线性层投影回原始的嵌入维度E 。与CNN通过堆叠局部卷积层来逐步扩大感受野不同，自注意力机制使得ViT能够直接建模全局关系，其注意力图甚至可以可视化，以理解模型关注的图像区域 。

**前馈网络（FFN）**：在MSA子层之后，每个图像块的嵌入向量会独立地通过一个前馈网络 。FFN通常由两个全连接层组成，中间使用一个非线性激活函数，如GELU（Gaussian Error Linear Unit）。FFN的作用是帮助模型学习输入数据中的非线性关系 。

**层归一化（LN）**：在每个编码器层中，LN被应用于MSA子层和FFN子层之前 。LN通过对每一层的激活进行归一化，有助于稳定训练过程并加速收敛。

**残差连接**：在每个MSA子层和FFN子层之后，都会添加一个残差连接（跳跃连接）。每个子层的输出会直接加回到其输入上。这有助于缓解梯度消失问题，使得训练更深的网络成为可能。

### 分类令牌
为了进行图像分类任务，通常会在输入到Transformer编码器之前，在图像块嵌入序列的开头添加一个可学习的分类令牌（令牌）。这个令牌与其他图像块嵌入一样，会通过所有的Transformer编码器层。在每个自注意力层中，令牌会与其他所有图像块嵌入进行交互，从而聚合来自整个图像的信息 。经过所有编码器层的处理后，令牌的最终隐藏状态被认为是整个输入图像的全局表示，并被用于分类任务 。使用专用的令牌，其灵感来源于BERT模型，使得ViT能够学习到专门为分类任务定制的图像全局表示。它与所有图像块的交互确保了最终的表示考虑了整个图像的上下文信息。

###  MLP头（分类头）
Transformer编码器最后一层的令牌的输出会被送入MLP头进行分类 。MLP头通常由一个或两个全连接层组成 。输出层的单元数等于图像分类任务中的类别数。通常，在输出层会使用Softmax激活函数，将输出转化为各个类别的概率分布 。概率最高的类别即为模型的预测结果。

##  示例
考虑一个简化的二分类任务：判断一个小的16x16像素的灰度图像包含的是“猫”还是“狗”。
1. **图像分块：** 我们选择4x4像素的图像块大小。这将把16x16的图像分割成(16/4) x (16/4) = 4x4 = 16个图像块。每个图像块的大小为4x4x1。
2. **扁平化：** 取第一个4x4的图像块，假设其像素值为：[0,0 ,0 ,0]    这个图像块被展平成一个16维的向量。
3. **线性投影（图像块嵌入）：** 这个16维的向量乘以一个可学习的权重矩阵（假设维度为16xE，为了简化，假设E=8）。这将得到一个8维的嵌入向量来表示这个图像块。例如，经过线性变换后，第一个图像块可能表示为向量：`[0.2, -0.5, 0.8, 0.1, -0.3, 0.6, 0.9, -0.2]`。这个过程对所有16个图像块重复进行。
4. **位置编码：** 由于我们有16个排列成4x4网格的图像块，我们将有16个位置编码向量，每个向量的维度为8。假设第一个图像块（位于(0,0)位置）的位置编码是`[0.1, 0.2, -0.1, 0.3, -0.2, 0.4, 0.1, -0.3]`。这个向量与图像块的嵌入向量逐元素相加：`[0.2+0.1, -0.5+0.2, 0.8-0.1, 0.1+0.3, -0.3-0.2, 0.6+0.4, 0.9+0.1, -0.2-0.3] = [0.3, -0.3, 0.7, 0.4, -0.5, 1.0, 1.0, -0.5]`。这个过程对所有16个图像块都进行。
5. **分类令牌：** 一个可学习的令牌（一个8维的向量，随机初始化）被添加到16个图像块嵌入序列的开头。现在我们有了一个包含17个嵌入向量的序列。
6. **Transformer编码器（简化为两个图像块）：** 为了简化，我们只关注单个自注意力头中前两个嵌入图像块（包括其位置编码）之间的交互。
    - **查询、键、值：** 每个8维的嵌入向量都乘以三个不同的可学习的权重矩阵（每个矩阵的维度为8x8），得到查询向量Q、键向量K和值向量V。假设对于第一个图像块，我们得到Q1、K1、V1，对于第二个图像块，我们得到Q2、K2、V2。
    - **注意力得分：** 计算第一个图像块和第二个图像块之间的注意力得分，通过计算Q1和K2的点积（Q1 · K2）。这个得分会被缩放并通过Softmax函数得到一个注意力权重，表示第一个图像块应该关注第二个图像块的程度。类似地，计算第一个图像块自身之间的注意力得分（Q1 · K1）。
    - **加权求和：** 第一个图像块在这个注意力头中的输出将是所有图像块的值向量的加权和（在我们的简化例子中，只有V1和V2），权重是上一步计算得到的注意力得分。
    - 这个过程对所有图像块对以及每个Transformer编码器层中的所有注意力头重复进行。令牌也参与这个注意力机制，关注所有图像块嵌入并被其关注。
7. **MLP头：** 经过多个Transformer编码器层的处理后，取出最终的令牌的8维表示。这个向量被送入一个简单的MLP头。假设我们的MLP头有一个大小为4的隐藏层和一个大小为2（对应“猫”和“狗”两个类别）的输出层。令牌的表示乘以第一个全连接层的权重矩阵（维度为8x4），应用一个非线性激活函数（例如ReLU），然后结果乘以第二个全连接层的权重矩阵（维度为4x2）。最后，对2维的输出应用Softmax函数，得到“猫”和“狗”这两个类别的概率。例如，输出可能是[0.9, 0.1]，表示图像有90%的概率包含“猫”。

**概念性计算结果：** 在这个简化的例子中，ViT模型通过分块、嵌入、注意力和分类的过程，判断输入图像很可能是一只“猫”。关键的计算包括线性变换（在嵌入和生成查询/键/值向量中）、点积（在计算注意力得分中）和加权求和（在注意力输出中）。可学习的参数（线性层中的权重矩阵和位置编码）在训练过程中进行调整，以使模型能够正确地分类图像。

## 结论
将图像分割成块、线性嵌入这些块、添加位置编码、通过包含多头自注意力和前馈网络的Transformer编码器堆栈处理序列，最后使用分类令牌和MLP头进行图像分类 。ViT的主要创新在于其使用自注意力机制直接捕获图像内的全局依赖关系。这

1. **ViT和CNN架构的比较**

| **特征**       | **视觉Transformer (ViT)** | **卷积神经网络 (CNN)**  |
| ------------ | ----------------------- | ----------------- |
| 核心机制         | 自注意力                    | 卷积                |
| 输入处理         | 图像分割成块，作为序列处理           | 使用卷积滤波器处理整个图像     |
| 全局上下文        | 从一开始就使用自注意力捕获           | 通过更深的网络层实现        |
| 位置信息         | 需要显式的位置编码               | 通过卷积层隐式地维护        |
| 归纳偏置         | 最小的归纳偏置                 | 对局部特征有很强的归纳偏置     |
| 数据需求         | 需要大型数据集才能获得最佳性能         | 在较小的数据集上也能表现良好    |
| 计算复杂度 (自注意力) | 相对于序列长度是二次的             | 相对于图像像素数是线性的      |
| 训练速度         | 通常较慢，因为自注意力计算成本高        | 通常更快              |
| 大型数据集上的性能    | 通常能达到最先进的结果             | 如果不仔细调整，性能可能会停滞不前 |

2. **单个ViT编码器层的组成部分**

| **组件**       | **描述**          | **功能**           |
| ------------ | --------------- | ---------------- |
| 层归一化         | 在多头自注意力之前应用     | 稳定训练并提高性能        |
| 多头自注意力 (MSA) | 计算所有图像块之间的注意力权重 | 建模全局关系和依赖性       |
| 残差连接         | 将MSA的输入添加到其输出   | 帮助训练更深的网络并防止梯度消失 |
| 层归一化         | 在前馈网络之前应用       | 进一步稳定训练          |
| 前馈网络 (FFN)   | 两个带有非线性激活的全连接层  | 学习数据中的非线性关系      |
| 残差连接         | 将FFN的输入添加到其输出   | 进一步帮助训练深层网络      |