import{_ as o,c as a,k as t,a as l,R as i,o as e}from"./chunks/framework.C6kDZlj-.js";const R=JSON.parse('{"title":"","description":"","frontmatter":{},"headers":[],"relativePath":"model/resnet.md","filePath":"model/resnet.md","lastUpdated":1733409473000}'),r={name:"model/resnet.md"},s=t("h3",{id:"_1-该模型的结构",tabindex:"-1"},[l("1. 该模型的结构 "),t("a",{class:"header-anchor",href:"#_1-该模型的结构","aria-label":'Permalink to "1. 该模型的结构"'},"​")],-1),n=t("p",null,[t("strong",null,"ResNet"),l("（Residual Network，残差网络）是由 Kaiming He 等人在 2015 年提出的一种深度卷积神经网络，通过引入 "),t("strong",null,"残差连接"),l(" 解决深层网络中的梯度消失和退化问题。")],-1),Q=t("h4",{id:"结构特点",tabindex:"-1"},[t("strong",null,"结构特点"),l(),t("a",{class:"header-anchor",href:"#结构特点","aria-label":'Permalink to "**结构特点**"'},"​")],-1),h=t("strong",null,"残差块（Residual Block）",-1),d=t("li",null,[l("核心是 "),t("strong",null,"跳跃连接（Shortcut Connection）"),l("。")],-1),T={class:"MathJax",jax:"SVG",display:"true",style:{direction:"ltr",display:"block","text-align":"center",margin:"1em 0",position:"relative"}},u={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.566ex"},xmlns:"http://www.w3.org/2000/svg",width:"16.889ex",height:"2.262ex",role:"img",focusable:"false",viewBox:"0 -750 7465 1000","aria-hidden":"true"},g=i('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(888,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1277,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1849,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2515.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(3571.6,0)"><path data-c="1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(4320.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(4709.6,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(5281.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(5892.8,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(6893,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g></g></g>',1),m=[g],c=t("mjx-assistive-mml",{unselectable:"on",display:"block",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",overflow:"hidden",width:"100%"}},[t("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[t("mi",null,"H"),t("mo",{stretchy:"false"},"("),t("mi",null,"x"),t("mo",{stretchy:"false"},")"),t("mo",null,"="),t("mi",null,"F"),t("mo",{stretchy:"false"},"("),t("mi",null,"x"),t("mo",{stretchy:"false"},")"),t("mo",null,"+"),t("mi",null,"x")])],-1),_=t("li",null,"其中，( F(x) ) 是学习的残差，( x ) 是输入，( H(x) ) 是输出。",-1),p=t("li",null,[t("strong",null,"堆叠结构"),l("： "),t("ul",null,[t("li",null,"多个残差块堆叠，构成更深的网络。")])],-1),x=t("li",null,[t("strong",null,"瓶颈块（Bottleneck Block）"),l("： "),t("ul",null,[t("li",null,"对于深层网络，通过 ( 1 \\times 1 ) 卷积减少维度，降低计算量。")])],-1),b=i('<h4 id="典型的-resnet-变种" tabindex="-1"><strong>典型的 ResNet 变种</strong> <a class="header-anchor" href="#典型的-resnet-变种" aria-label="Permalink to &quot;**典型的 ResNet 变种**&quot;">​</a></h4><ul><li>ResNet-18、ResNet-34：基于基本残差块。</li><li>ResNet-50、ResNet-101：基于瓶颈残差块。</li></ul><hr><h3 id="_2-该模型详细的实现过程" tabindex="-1">2. 该模型详细的实现过程 <a class="header-anchor" href="#_2-该模型详细的实现过程" aria-label="Permalink to &quot;2. 该模型详细的实现过程&quot;">​</a></h3><ol><li><p><strong>输入层</strong>：</p><ul><li>输入图像，尺寸通常为 ( 224 \\times 224 \\times 3 )。</li><li>使用一个 ( 7 \\times 7 ) 的卷积核进行特征提取，步长为 2，后接最大池化。</li></ul></li><li><p><strong>主干网络</strong>：</p><ul><li>包括多个残差块，每个块内包含两次卷积（或三次卷积，瓶颈结构），跳跃连接将输入直接与输出相加。</li><li>每个阶段降低分辨率（步长为 2），特征图尺寸逐渐减小，通道数逐渐增加。</li></ul></li><li><p><strong>全局平均池化</strong>：</p><ul><li>将最后一层的特征图进行全局平均池化，输出一个特征向量。</li></ul></li><li><p><strong>全连接层</strong>：</p><ul><li>通过全连接层（或线性层）输出最终分类结果。</li></ul></li><li><p><strong>损失函数</strong>：</p><ul><li>使用交叉熵损失函数进行分类任务训练。</li></ul></li></ol><hr><h3 id="_3-该模型的详细架构图" tabindex="-1">3. 该模型的详细架构图 <a class="header-anchor" href="#_3-该模型的详细架构图" aria-label="Permalink to &quot;3. 该模型的详细架构图&quot;">​</a></h3><h4 id="以-resnet-50-为例" tabindex="-1"><strong>以 ResNet-50 为例</strong> <a class="header-anchor" href="#以-resnet-50-为例" aria-label="Permalink to &quot;**以 ResNet-50 为例**&quot;">​</a></h4><ol><li><p><strong>输入层</strong>：</p><ul><li>卷积 ( 7 \\times 7 ), 步长 2, 输出 ( 112 \\times 112 \\times 64 )。</li><li>最大池化 ( 3 \\times 3 ), 步长 2, 输出 ( 56 \\times 56 \\times 64 )。</li></ul></li><li><p><strong>主干网络</strong>：</p><ul><li><strong>Stage 1</strong>: 3 个瓶颈残差块，输出 ( 56 \\times 56 \\times 256 )。</li><li><strong>Stage 2</strong>: 4 个瓶颈残差块，输出 ( 28 \\times 28 \\times 512 )。</li><li><strong>Stage 3</strong>: 6 个瓶颈残差块，输出 ( 14 \\times 14 \\times 1024 )。</li><li><strong>Stage 4</strong>: 3 个瓶颈残差块，输出 ( 7 \\times 7 \\times 2048 )。</li></ul></li><li><p><strong>分类层</strong>：</p><ul><li>全局平均池化：输出 ( 1 \\times 1 \\times 2048 )。</li><li>全连接层：根据类别数输出。</li></ul></li></ol><hr><h3 id="_4-该模型的前世今生-谁创造的-为什么创造的" tabindex="-1">4. 该模型的前世今生（谁创造的，为什么创造的） <a class="header-anchor" href="#_4-该模型的前世今生-谁创造的-为什么创造的" aria-label="Permalink to &quot;4. 该模型的前世今生（谁创造的，为什么创造的）&quot;">​</a></h3><h4 id="创造者" tabindex="-1"><strong>创造者</strong>： <a class="header-anchor" href="#创造者" aria-label="Permalink to &quot;**创造者**：&quot;">​</a></h4><ul><li>Kaiming He、Xiangyu Zhang、Shaoqing Ren、Jian Sun 于 2015 年提出，发表在 CVPR。</li></ul><h4 id="创作背景" tabindex="-1"><strong>创作背景</strong>： <a class="header-anchor" href="#创作背景" aria-label="Permalink to &quot;**创作背景**：&quot;">​</a></h4><ul><li>深度网络理论上能学习更复杂的特征，但实践中，随着网络加深，出现了 <strong>梯度消失</strong> 和 <strong>退化问题</strong>。</li><li>深层网络在训练过程中，性能反而下降。</li></ul><h4 id="核心思想" tabindex="-1"><strong>核心思想</strong>： <a class="header-anchor" href="#核心思想" aria-label="Permalink to &quot;**核心思想**：&quot;">​</a></h4><p>通过 <strong>残差连接</strong> 跳过一些层的直接路径，使网络更容易优化。</p><hr><h3 id="_5-该模型之前的用处和现在的用处" tabindex="-1">5. 该模型之前的用处和现在的用处 <a class="header-anchor" href="#_5-该模型之前的用处和现在的用处" aria-label="Permalink to &quot;5. 该模型之前的用处和现在的用处&quot;">​</a></h3><h4 id="之前的用处" tabindex="-1"><strong>之前的用处</strong>： <a class="header-anchor" href="#之前的用处" aria-label="Permalink to &quot;**之前的用处**：&quot;">​</a></h4><ul><li>图像分类（ImageNet）。</li><li>目标检测（通过 Faster R-CNN 集成 ResNet）。</li><li>语义分割（如 PSPNet 和 DeepLab 使用 ResNet 作为主干网络）。</li></ul><h4 id="现在的用处" tabindex="-1"><strong>现在的用处</strong>： <a class="header-anchor" href="#现在的用处" aria-label="Permalink to &quot;**现在的用处**：&quot;">​</a></h4><ul><li><strong>迁移学习</strong>：用 ResNet 提取特征，在各种领域中广泛应用。</li><li><strong>生成任务</strong>：如 GAN 的生成器和判别器中使用 ResNet。</li><li><strong>时间序列分析</strong>：ResNet 的残差思想被移植到序列任务。</li></ul><hr><h3 id="_6-该模型的主要算法" tabindex="-1">6. 该模型的主要算法 <a class="header-anchor" href="#_6-该模型的主要算法" aria-label="Permalink to &quot;6. 该模型的主要算法&quot;">​</a></h3><ul><li><strong>残差学习</strong>： <ul><li>学习残差 ( F(x) )，使得优化问题从学习 ( H(x) ) 转为学习 ( F(x) = H(x) - x )，降低了优化难度。</li></ul></li><li><strong>跳跃连接</strong>： <ul><li>直接将输入 ( x ) 加入到输出 ( F(x) ) 上，避免梯度消失问题。</li></ul></li><li><strong>瓶颈结构</strong>： <ul><li>通过 ( 1 \\times 1 ) 卷积压缩通道数，减少计算量。</li></ul></li></ul><hr><h3 id="_7-具体的算法过程" tabindex="-1">7. 具体的算法过程 <a class="header-anchor" href="#_7-具体的算法过程" aria-label="Permalink to &quot;7. 具体的算法过程&quot;">​</a></h3><h4 id="单个残差块过程" tabindex="-1"><strong>单个残差块过程</strong>： <a class="header-anchor" href="#单个残差块过程" aria-label="Permalink to &quot;**单个残差块过程**：&quot;">​</a></h4><ol><li>输入：特征图 ( x )。</li><li>第一层卷积：( 3 \\times 3 ) 卷积，输出 ( F_1(x) )。</li><li>第二层卷积：( 3 \\times 3 ) 卷积，输出 ( F_2(x) )。</li><li>残差连接：输出 ( H(x) = F_2(x) + x )。</li><li>激活：ReLU 激活函数。</li></ol><h4 id="网络训练" tabindex="-1"><strong>网络训练</strong>： <a class="header-anchor" href="#网络训练" aria-label="Permalink to &quot;**网络训练**：&quot;">​</a></h4><ol><li>前向传播：计算损失。</li><li>反向传播：通过跳跃连接将梯度更容易传递到深层。</li><li>参数更新：通过优化器（如 SGD）。</li></ol><hr><h3 id="_8-是否存在特征提取具体-怎么提取的" tabindex="-1">8. 是否存在特征提取具体，怎么提取的 <a class="header-anchor" href="#_8-是否存在特征提取具体-怎么提取的" aria-label="Permalink to &quot;8. 是否存在特征提取具体，怎么提取的&quot;">​</a></h3><h4 id="特征提取具体" tabindex="-1"><strong>特征提取具体</strong>： <a class="header-anchor" href="#特征提取具体" aria-label="Permalink to &quot;**特征提取具体**：&quot;">​</a></h4><ul><li>每个卷积层提取局部特征。</li><li>随着网络加深，特征逐渐从低级（边缘、纹理）转为高级（语义、形状）。</li></ul><h4 id="提取方法" tabindex="-1"><strong>提取方法</strong>： <a class="header-anchor" href="#提取方法" aria-label="Permalink to &quot;**提取方法**：&quot;">​</a></h4><ul><li><strong>浅层卷积</strong>：提取低级特征（如边缘）。</li><li><strong>深层卷积</strong>：通过多个残差块提取高层语义特征。</li><li><strong>全局平均池化</strong>：将所有特征图转化为全局特征向量。</li></ul><hr><h3 id="_9-如果我要修改该模型的话改哪里-在哪里怎么实现的" tabindex="-1">9. 如果我要修改该模型的话改哪里，在哪里怎么实现的？ <a class="header-anchor" href="#_9-如果我要修改该模型的话改哪里-在哪里怎么实现的" aria-label="Permalink to &quot;9. 如果我要修改该模型的话改哪里，在哪里怎么实现的？&quot;">​</a></h3><h4 id="可以修改的部分" tabindex="-1"><strong>可以修改的部分</strong>： <a class="header-anchor" href="#可以修改的部分" aria-label="Permalink to &quot;**可以修改的部分**：&quot;">​</a></h4><ol><li><strong>卷积层参数</strong>： <ul><li>改变卷积核大小、步长或通道数。</li></ul></li><li><strong>残差连接</strong>： <ul><li>替换跳跃连接为加权连接。</li></ul></li><li><strong>网络深度</strong>： <ul><li>增加或减少残差块数量。</li></ul></li><li><strong>激活函数</strong>： <ul><li>替换 ReLU 为 LeakyReLU 或 GELU。</li></ul></li><li><strong>损失函数</strong>： <ul><li>替换交叉熵损失为焦点损失（Focal Loss）。</li></ul></li><li><strong>优化器</strong>： <ul><li>替换 SGD 为 AdamW。</li></ul></li></ol><h4 id="如何实现修改" tabindex="-1"><strong>如何实现修改</strong>： <a class="header-anchor" href="#如何实现修改" aria-label="Permalink to &quot;**如何实现修改**：&quot;">​</a></h4><ul><li><strong>使用深度学习框架（如 PyTorch）</strong>： <ul><li>在代码中调整 <code>nn.Conv2d</code> 参数，或修改残差连接逻辑。</li></ul></li><li><strong>修改模型结构</strong>： <ul><li>改变 <code>forward</code> 方法，加入自定义逻辑。</li></ul></li><li><strong>重新训练</strong>： <ul><li>在修改后，用新数据重新训练模型。</li></ul></li></ul>',44);function H(f,k,q,P,w,y){return e(),a("div",null,[s,n,Q,t("ul",null,[t("li",null,[h,l("： "),t("ul",null,[d,t("li",null,[l("每个残差块直接将输入与经过两次卷积的特征相加："),t("mjx-container",T,[(e(),a("svg",u,m)),c])]),_])]),p,x]),b])}const L=o(r,[["render",H]]);export{R as __pageData,L as default};
